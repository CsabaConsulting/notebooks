{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "798Rf5S52cJM"
   },
   "source": [
    "## How-to guide for Cloud Spend use-case on Abacus.AI platform\n",
    "This notebook provides you with a hands on environment to build an cloud spend alert model using the Abacus.AI Python Client Library.\n",
    "\n",
    "We'll be using the [Cloud Spend Dataset](https://s3.amazonaws.com//realityengines.exampledatasets/cloud_operations/cloud_spend.csv), which, as the name suggests, contains information about cloud spends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--mZuKj93z18"
   },
   "source": [
    "1. Install the Abacus.AI library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1rd5a6Hyx0a",
    "outputId": "b88ad60f-3b7e-4c23-f1c8-26c725eee0e6"
   },
   "outputs": [],
   "source": [
    "!pip install abacusai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aqte63CZ351O"
   },
   "source": [
    "We'll also import pandas and pprint tools for visualization in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSaBA5WzzoNl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # A tool we'll use to download and preview CSV files\n",
    "import pprint # A tool to pretty print dictionary outputs\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_0en-0w3-H8"
   },
   "source": [
    "2. Add your Abacus.AI [API Key](https://abacus.ai/app/profile/apikey) generated using the API dashboard as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vxWCivxzqSo"
   },
   "outputs": [],
   "source": [
    "#@title Abacus.AI API Key\n",
    "\n",
    "api_key = ''  #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T2t6nbU4Jfv"
   },
   "source": [
    "3. Import the Abacus.AI library and instantiate a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQdlAX0uztIT",
    "outputId": "51718bcc-b618-4eab-c8d1-3cdfb13dc293"
   },
   "outputs": [],
   "source": [
    "from abacusai import ApiClient\n",
    "client = ApiClient(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czQai9Qn4Lom"
   },
   "source": [
    "## 1. Create a Project\n",
    "\n",
    "Abacus.AI projects are containers that have datasets and trained models. By specifying a business **Use Case**, Abacus.AI tailors the deep learning algorithms to produce the best performing model catered specifically for your data.\n",
    "\n",
    "We'll call the `list_use_cases` method to retrieve a list of the Use Cases currently available on the Abacus.AI platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1mop7blz7P2",
    "outputId": "84dcd395-5330-43ce-ffef-431000938ff5"
   },
   "outputs": [],
   "source": [
    "client.list_use_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyNpJwSC4OWp"
   },
   "source": [
    "In this notebook, we're going to create a cloud spend alert model using the Cloud Spend dataset. The 'OPERATIONS_CLOUD' use case is best tailored for this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwczQf84z_6A"
   },
   "outputs": [],
   "source": [
    "#@title Abacus.AI Use Case\n",
    "\n",
    "use_case = 'OPERATIONS_CLOUD'  #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vj6gAnf4X54"
   },
   "source": [
    "By calling the `describe_use_case_requirements` method we can view what datasets are required for this use_case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t81mQJzz0JPX",
    "outputId": "0a94ae51-0fdd-4e73-c142-2c198ee25199"
   },
   "outputs": [],
   "source": [
    "for requirement in client.describe_use_case_requirements(use_case):\n",
    "  pp.pprint(requirement.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi9sWxXS4c5x"
   },
   "source": [
    "Finally, let's create the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7PREqP80Nn7",
    "outputId": "f17d3c91-f8be-4865-9c5e-8361832c6f8e"
   },
   "outputs": [],
   "source": [
    "cloud_project = client.create_project(name='Cloud Spend Project', use_case=use_case)\n",
    "cloud_project.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMfUiKAwrgCy"
   },
   "source": [
    "**Note: When feature_groups_enabled is True then the use case supports feature groups (collection of ML features). Feature groups are created at the organization level and can be tied to a project to further use it for training ML models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QD-SqIiQ4hQK"
   },
   "source": [
    "## 2. Add Datasets to your Project\n",
    "\n",
    "Abacus.AI can read datasets directly from `AWS S3` or `Google Cloud Storage` buckets, otherwise you can also directly upload and store your datasets with Abacus.AI. For this notebook, we will have Abacus.AI read the datasets directly from a public S3 bucket's location.\n",
    "\n",
    "We are using one dataset for this notebook. We'll tell Abacus.AI how the dataset should be used when creating it by tagging the dataset with a special Abacus.AI **Dataset Type**.\n",
    "- [Cloud Spend Dataset](https://s3.amazonaws.com//realityengines.exampledatasets/cloud_operations/cloud_spend.csv) (**TIMESERIES**): \n",
    "This dataset contains information about a company's cloud expenses, including the start date, service, and log cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHDCJryc5CNj"
   },
   "source": [
    "### Add the dataset to Abacus.AI\n",
    "\n",
    "First we'll use Pandas to preview the file, then add it to Abacus.AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "LdUfs-Zp15Xp",
    "outputId": "8ac4db09-2589-41aa-e2d3-58ff00e8ce8c"
   },
   "outputs": [],
   "source": [
    "pd.read_csv('https://s3.amazonaws.com//realityengines.exampledatasets/cloud_operations/cloud_spend.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-3GYhCn5DQg"
   },
   "source": [
    "Using the Create Dataset API, we can tell Abacus.AI the public S3 URI of where to find the datasets. We will also give each dataset a Refresh Schedule, which tells Abacus.AI when it should refresh the dataset (take an updated/latest copy of the dataset).\n",
    "\n",
    "If you're unfamiliar with Cron Syntax, Crontab Guru can help translate the syntax back into natural language: [https://crontab.guru/#0_12_\\*_\\*_\\*](https://crontab.guru/#0_12_*_*_*)\n",
    "\n",
    "**Note: This cron string will be evaluated in UTC time zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rx2-HfHagdr4"
   },
   "outputs": [],
   "source": [
    "cloud_dataset = client.create_dataset_from_file_connector(name='Test Cloud Spend Data', table_name='Test_Cloud_Spend_Data' ,\n",
    "                                     location='s3://realityengines.exampledatasets/cloud_operations/cloud_spend.csv',\n",
    "                                     refresh_schedule='0 12 * * *')\n",
    "datasets = [cloud_dataset]\n",
    "for dataset in datasets:\n",
    "    dataset.wait_for_inspection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4t8ytsQrw6o"
   },
   "source": [
    "## 3. Create Feature Groups and add them to your Project\n",
    "\n",
    "Datasets are created at the organization level and can be used to create feature groups as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w08H16C1auPA"
   },
   "outputs": [],
   "source": [
    "feature_group = client.create_feature_group(table_name='test_cloud_spends_alert',sql='SELECT * FROM Test_Cloud_Spend_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCrg5R6kr6pZ"
   },
   "source": [
    "Adding Feature Group to the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svyfGlK2bNwK"
   },
   "outputs": [],
   "source": [
    "client.add_feature_group_to_project(feature_group_id=feature_group.feature_group_id,project_id = cloud_project.project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAAAnOz8r_MD"
   },
   "source": [
    "Setting the Feature Group type according to the use case requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOj0Er7GbNzS"
   },
   "outputs": [],
   "source": [
    "client.set_feature_group_type(feature_group_id=feature_group.feature_group_id, project_id = cloud_project.project_id, feature_group_type= \"TIMESERIES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l-Hz4zHsLGo"
   },
   "source": [
    "Check current Feature Group schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-MyrAK0bN1z",
    "outputId": "4efe6c74-4c0b-4baa-f4a7-341bc5a3c182"
   },
   "outputs": [],
   "source": [
    "client.get_feature_group_schema(feature_group_id=feature_group.feature_group_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp-N6ZhZ5KeI"
   },
   "source": [
    "#### For each **Use Case**, there are special **Column Mappings** that must be applied to a column to fulfill use case requirements. We can find the list of available **Column Mappings** by calling the *Describe Use Case Requirements* API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SeqQIzWZCER",
    "outputId": "b0ec8445-93f3-45a1-91ba-6d083095ffd0"
   },
   "outputs": [],
   "source": [
    "client.describe_use_case_requirements(use_case)[0].allowed_feature_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOdbvCkdaHz3",
    "outputId": "e7ae09a9-872f-4803-8459-a21d4ccc9cdb"
   },
   "outputs": [],
   "source": [
    "client.set_feature_mapping(project_id = cloud_project.project_id,feature_group_id= feature_group.feature_group_id, feature_name='UsageStartDate',feature_mapping='DATE')\n",
    "client.set_feature_mapping(project_id = cloud_project.project_id,feature_group_id= feature_group.feature_group_id, feature_name='service',feature_mapping='SERVICE_ID')\n",
    "client.set_feature_mapping(project_id = cloud_project.project_id,feature_group_id= feature_group.feature_group_id, feature_name='LogCost',feature_mapping='VALUE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1Qh4mTvs1Bn"
   },
   "source": [
    "Now that we've our feature groups assigned, we're almost ready to train a model!\n",
    "\n",
    "To be sure that our project is ready to go, let's call project.validate to confirm that all the project requirements have been met:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtB15JT9ZrVW",
    "outputId": "e4acf223-cb2f-4724-88c4-824a8074b097"
   },
   "outputs": [],
   "source": [
    "cloud_project.validate(feature_group_ids=[feature_group.feature_group_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-FIyjbX5x_b"
   },
   "source": [
    "## 4. Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrYxMfrP52g5"
   },
   "source": [
    "For each **Use Case**, Abacus.AI has a bunch of options for training. We can call the *Get Training Config Options* API to see the available options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxnw7no0Zvaj",
    "outputId": "a3c45eff-3d06-46ed-e431-0d3a0fb4e17c"
   },
   "outputs": [],
   "source": [
    "cloud_project.get_training_config_options(feature_group_ids=[feature_group.feature_group_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1oopEwk54xD"
   },
   "source": [
    "In this notebook, we'll just train with the default options, but definitely feel free to experiment, especially if you have familiarity with Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2Uy6IgIZy8I",
    "outputId": "f65281ee-f0ce-4f9a-ed75-a300bea13ac2"
   },
   "outputs": [],
   "source": [
    "cloud_model = cloud_project.train_model(training_config={},feature_group_ids=[feature_group.feature_group_id])\n",
    "cloud_model.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfTu53sI59SJ"
   },
   "source": [
    "After we start training the model, we can call this blocking call that routinely checks the status of the model until it is trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsSIe5Gbb5El",
    "outputId": "f9f7b2cb-c8ec-4af9-87f0-5bb812bcd9f7"
   },
   "outputs": [],
   "source": [
    "cloud_model.wait_for_full_automl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfaB4_k5tKwB"
   },
   "source": [
    "**Note that model training might take some minutes to some hours depending upon the size of datasets, complexity of the models being trained and a variety of other factors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uv9RckJr6AFQ"
   },
   "source": [
    "## **Checkpoint** [Optional]\n",
    "As model training can take an hours to complete, your page could time out or you might end up hitting the refresh button, this section helps you restore your progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-O6jEpvi6Dnm"
   },
   "outputs": [],
   "source": [
    "!pip install abacusai\n",
    "import pandas as pd\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "api_key = ''  #@param {type: \"string\"}\n",
    "from abacusai import ApiClient\n",
    "client = ApiClient(api_key)\n",
    "cloud_project = next(project for project in client.list_projects() if project.name == 'Cloud Spend Project')\n",
    "cloud_model = cloud_project.list_models()[-1]\n",
    "cloud_model.wait_for_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5CeWns16Of6"
   },
   "source": [
    "## Evaluate your Model Metrics\n",
    "\n",
    "After your model is done training you can inspect the model's quality by reviewing the model's metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssQeRUTBaZlx",
    "outputId": "0e878fd2-400f-49b1-8bef-15454cbddffb"
   },
   "outputs": [],
   "source": [
    "pp.pprint(cloud_model.get_metrics().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICiIimPs6Qnn"
   },
   "source": [
    "To get a better understanding on what these metrics mean, visit our [documentation](https://abacus.ai/app/help/useCases/OPERATIONS_CLOUD/training) page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SXHfF346Wyb"
   },
   "source": [
    "## 5. Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcF-p-9u6anF"
   },
   "source": [
    "After the model has been trained, we need to create a deployment token for authenticating prediction requests. This token is only authorized to predict on deployments in this project, so it's safe to embed this token inside of a user-facing application or website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dRybQyY6cilt",
    "outputId": "3cf78c3a-aaf6-44ea-95aa-a46e7a7a5250"
   },
   "outputs": [],
   "source": [
    "deployment_token = cloud_project.create_deployment_token().deployment_token\n",
    "deployment_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II_0L3_c6XFU"
   },
   "source": [
    "Now we need to deploy the model to be able to start making predictions. Deploying a model will reserve cloud resources to host the model for Realtime and/or batch predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGgpYiyscQ8T",
    "outputId": "9e4f2e25-a614-49fe-eb00-6f78a1e9ba7d"
   },
   "outputs": [],
   "source": [
    "cloud_deployment = client.create_deployment(name='Cloud Spend Deployment', model_id=cloud_model.model_id,description='Cloud Spend Deployment')\n",
    "cloud_deployment.wait_for_deployment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyAkodtr6czT"
   },
   "source": [
    "## 6. Predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4g4bWvWU6dMo"
   },
   "source": [
    "Now that you have an active deployment and a deployment token to authenticate requests, you can make the `get_anomalies` API call below.\n",
    "\n",
    "This command will return information about a company's cloud spending, including the threshold, service, and anomalies. The anomaly detection would be performed using the data in the Cloud Spend dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMfeKzvdcmHv",
    "outputId": "079bd182-9ded-4ebb-8e5c-5c2d5d20f27d"
   },
   "outputs": [],
   "source": [
    "ApiClient().get_anomalies(deployment_token=deployment_token, \n",
    "               deployment_id=cloud_deployment.deployment_id)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cloud Spend Alerts Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a444a4e97a0365a8a152d6b3286685b7a3c540a2b13f4d560bd58f028f75a5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
